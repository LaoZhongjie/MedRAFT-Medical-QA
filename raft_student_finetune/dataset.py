"""
æ•°æ®é›†å¤„ç†æ¨¡å—
è´Ÿè´£åŠ è½½ã€é¢„å¤„ç†RAFTæ•°æ®é›†
"""
import json
import torch
from typing import Dict, List, Any
from torch.utils.data import Dataset
from transformers import PreTrainedTokenizer
from config import DataConfig
import textwrap

class RAFTDataset(Dataset):
    """RAFTçŸ¥è¯†è’¸é¦æ•°æ®é›†"""
    
    def __init__(
        self,
        data_path: str,
        tokenizer: PreTrainedTokenizer,
        data_config: DataConfig,
        max_length: int = 4096
    ):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            tokenizer: åˆ†è¯å™¨
            data_config: æ•°æ®é…ç½®
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.tokenizer = tokenizer
        self.data_config = data_config
        self.max_length = max_length
        self.normal_count = 0
        self.problem_count = 0
    
        # åŠ è½½æ•°æ®
        with open(data_path, 'r', encoding='utf-8') as f:
            self.raw_data = json.load(f)
        
        print(f"åŠ è½½æ•°æ®é›†: {len(self.raw_data)} æ¡æ ·æœ¬")
    
    def __len__(self) -> int:
        return len(self.raw_data)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """è·å–å•æ¡æ•°æ®ï¼Œè¿‡æ»¤æ— æ•ˆæ ·æœ¬"""
        try:
            item = self.raw_data[idx]
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            if not all(key in item for key in ['question', 'documents', 'teacher_answer']):
                raise ValueError(f"æ ·æœ¬ {idx} ç¼ºå°‘å¿…è¦å­—æ®µ")
            
            if len(item['teacher_answer'].strip()) < 10:
                raise ValueError(f"æ ·æœ¬ {idx} teacher_answerè¿‡çŸ­")
            
            # æ„å»ºè¾“å…¥prompt
            prompt = self._build_prompt(item)
            
            # è·å–æ•™å¸ˆç­”æ¡ˆ
            teacher_answer = item['teacher_answer']
            
            # æ„å»ºå®Œæ•´æ–‡æœ¬ç”¨äºè®­ç»ƒ
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç–—é¡¾é—®åŠ©æ‰‹,æ“…é•¿ç»“æ„åŒ–æ¨ç†å’ŒåŒ»ç–—å»ºè®®ã€‚"},
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": teacher_answer}
            ]
            
            # ä½¿ç”¨tokenizerçš„chat_template
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
            
            # ç¼–ç 
            encoded = self.tokenizer(
                text,
                max_length=self.max_length,
                truncation=True,
                padding=False,
                return_tensors=None
            )
            
            input_ids = encoded['input_ids']
            attention_mask = encoded['attention_mask']
            
            # åˆ›å»ºlabels (ä»…è®¡ç®—assistantå›ç­”éƒ¨åˆ†çš„loss)
            labels = self._create_labels(messages, input_ids)
            
            # éªŒè¯labelsæœ‰æ•ˆæ€§
            valid_labels_count = sum(1 for x in labels if x != -100)
            
            # ğŸ”´ å…³é”®ä¿®æ”¹ï¼šå¦‚æœæœ‰æ•ˆlabelså¤ªå°‘ï¼Œè·³è¿‡è¿™ä¸ªæ ·æœ¬
            if valid_labels_count < 100:
                self.problem_count += 1
                print(f"âŒ è·³è¿‡æ ·æœ¬ {idx}: æœ‰æ•ˆlabels={valid_labels_count} (ç¬¬{self.problem_count}ä¸ªé—®é¢˜æ ·æœ¬)")
                raise ValueError(f"æœ‰æ•ˆlabelsä¸è¶³: {valid_labels_count}")
            
            # åªæœ‰æ­£å¸¸æ ·æœ¬æ‰ä¼šæ‰§è¡Œåˆ°è¿™é‡Œ
            self.normal_count += 1
            if self.normal_count <= 10:  # åªè®°å½•å‰10ä¸ªæ­£å¸¸æ ·æœ¬
                print(f"âœ… æ­£å¸¸æ ·æœ¬ {idx}: é•¿åº¦={len(input_ids)}, æœ‰æ•ˆlabels={valid_labels_count}")
            
            return {
                'input_ids': torch.tensor(input_ids, dtype=torch.long),
                'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
                'labels': torch.tensor(labels, dtype=torch.long)
            }
            
        except Exception as e:
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©DataLoaderè·³è¿‡è¿™ä¸ªæ ·æœ¬
            raise IndexError(f"è·³è¿‡æ ·æœ¬ {idx}: {e}")
    
    def _build_prompt(self, item: Dict[str, Any]) -> str:
        """
        æ„å»ºè¾“å…¥prompt
        
        Args:
            item: å•æ¡æ•°æ®æ ·æœ¬
            
        Returns:
            æ ¼å¼åŒ–çš„promptå­—ç¬¦ä¸²
        """
        question = item['question']
        documents = item['documents']
        
        # åˆå¹¶æ‰€æœ‰æ–‡æ¡£å†…å®¹
        doc_texts = []
        for i, doc in enumerate(documents, 1):
            content = doc['content']
            doc_texts.append(f"[æ–‡æ¡£{i}]{content}")
        
        combined_docs = "\n\n".join(doc_texts)
        
        # æ„å»ºå®Œæ•´prompt
        prompt = textwrap.dedent(f"""
            ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„ä¸­æ–‡åŒ»å­¦ä¸“å®¶ï¼ŒåŸºäºä¸‹é¢çš„çŸ¥è¯†åº“æ–‡æ¡£ï¼Œå›ç­”æ‚£è€…çš„åŒ»å­¦é—®é¢˜ã€‚è¦æ±‚ä¸¥æ ¼éµå®ˆä¸‹åˆ—æ ¼å¼ä¸è§„åˆ™ã€‚

            çŸ¥è¯†åº“æ–‡æ¡£:
            {combined_docs}

            é—®é¢˜: {question}

            è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºä½ çš„å›ç­”:
            - é—®é¢˜: [é‡å¤ç”¨æˆ·é—®é¢˜]
            - å‡è®¾/å·²çŸ¥ä¿¡æ¯: [åˆ—å‡ºä»æ–‡æ¡£ä¸­æå–çš„ç›¸å…³ä¿¡æ¯]
            - CoTæ¨ç†: [é€æ­¥æ¨ç†è¿‡ç¨‹ï¼Œå¯ä»¥åŒ…å«å¤šä¸ªæ­¥éª¤]
            - åˆæ­¥è¯Šæ–­å»ºè®®(å«ä¸ç¡®å®šåº¦): [ç»™å‡ºå»ºè®®åŠç½®ä¿¡åº¦]
            - è¯æ®å¼•ç”¨: [å¼•ç”¨æ”¯æŒä½ ç»“è®ºçš„æ–‡æ¡£ç‰‡æ®µ+source]
            - ä¸è¶³ä¿¡æ¯ä¸åç»­å»ºè®®: [æŒ‡å‡ºç¼ºå¤±çš„ä¿¡æ¯]
            """)
        return prompt
    
    def _create_labels(self, messages: List[Dict], input_ids: List[int]) -> List[int]:
        """
        åˆ›å»ºlabels,åªå¯¹assistantçš„å›ç­”è®¡ç®—loss
        """
        # å…ˆå…¨éƒ¨è®¾ä¸º-100(å¿½ç•¥)
        labels = [-100] * len(input_ids)
        
        try:
            # æ‰¾åˆ°assistantå›ç­”çš„å†…å®¹
            assistant_content = messages[-1]['content']
            assistant_tokens = self.tokenizer.encode(
                assistant_content,
                add_special_tokens=False
            )
            
            # åœ¨input_idsä¸­æŸ¥æ‰¾assistant_tokensçš„ä½ç½®
            found = False
            for i in range(len(input_ids) - len(assistant_tokens) + 1):
                if input_ids[i:i+len(assistant_tokens)] == assistant_tokens:
                    # æ‰¾åˆ°äº†,è®¾ç½®ä»åŒ¹é…ä½ç½®å¼€å§‹å¾€åçš„æ‰€æœ‰labels
                    for j in range(i, len(input_ids)):
                        labels[j] = input_ids[j]
                    found = True
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ°å®Œå…¨åŒ¹é…ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…
            if not found and len(assistant_tokens) > 10:
                # å°è¯•åŒ¹é…å‰20ä¸ªtoken
                partial_tokens = assistant_tokens[:20]
                for i in range(len(input_ids) - len(partial_tokens) + 1):
                    if input_ids[i:i+len(partial_tokens)] == partial_tokens:
                        # æ‰¾åˆ°äº†éƒ¨åˆ†åŒ¹é…ï¼Œè®¾ç½®ä»åŒ¹é…ä½ç½®å¼€å§‹å¾€åçš„æ‰€æœ‰labels
                        for j in range(i, len(input_ids)):
                            labels[j] = input_ids[j]
                        found = True
                        break
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            if not found:
                # è®¾ç½®å1/2çš„tokenä¸ºæœ‰æ•ˆï¼ˆåŸºäºç»éªŒï¼‰
                start_pos = len(input_ids) // 2
                for i in range(start_pos, len(input_ids)):
                    labels[i] = input_ids[i]
                    
        except Exception as e:
            print(f"âŒ labelsåˆ›å»ºå¼‚å¸¸: {e}")
            # ç´§æ€¥å¤‡ç”¨ï¼šè®¾ç½®ååŠéƒ¨åˆ†ä¸ºæœ‰æ•ˆ
            start_pos = len(input_ids) // 2
            for i in range(start_pos, len(input_ids)):
                labels[i] = input_ids[i]
        
        return labels

def create_collate_fn(tokenizer: PreTrainedTokenizer, max_length: int):
    """
    åˆ›å»ºæ•°æ®æ•´ç†å‡½æ•°
    
    Args:
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§é•¿åº¦
        
    Returns:
        collateå‡½æ•°
    """
    def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """æ•´ç†batchæ•°æ®"""
        # è·å–batchä¸­çš„æœ€å¤§é•¿åº¦
        max_len = min(
            max([len(item['input_ids']) for item in batch]),
            max_length
        )
        
        # Padding
        input_ids = []
        attention_mask = []
        labels = []
        
        for item in batch:
            seq_len = len(item['input_ids'])
            
            if seq_len > max_len:
                # æˆªæ–­
                input_ids.append(item['input_ids'][:max_len])
                attention_mask.append(item['attention_mask'][:max_len])
                labels.append(item['labels'][:max_len])
            else:
                # Padding
                pad_len = max_len - seq_len
                input_ids.append(
                    torch.cat([
                        item['input_ids'],
                        torch.full((pad_len,), tokenizer.pad_token_id, dtype=torch.long)
                    ])
                )
                attention_mask.append(
                    torch.cat([
                        item['attention_mask'],
                        torch.zeros(pad_len, dtype=torch.long)
                    ])
                )
                labels.append(
                    torch.cat([
                        item['labels'],
                        torch.full((pad_len,), -100, dtype=torch.long)
                    ])
                )
        
        return {
            'input_ids': torch.stack(input_ids),
            'attention_mask': torch.stack(attention_mask),
            'labels': torch.stack(labels)
        }
    
    return collate_fn


def split_dataset(dataset: RAFTDataset, validation_split: float = 0.2, seed: int = 42):
    """
    åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    
    Args:
        dataset: å®Œæ•´æ•°æ®é›†
        validation_split: éªŒè¯é›†æ¯”ä¾‹
        seed: éšæœºç§å­
        
    Returns:
        (train_dataset, eval_dataset)
    """
    from torch.utils.data import random_split
    
    total_size = len(dataset)
    eval_size = int(total_size * validation_split)
    train_size = total_size - eval_size
    
    generator = torch.Generator().manual_seed(seed)
    train_dataset, eval_dataset = random_split(
        dataset,
        [train_size, eval_size],
        generator=generator
    )
    
    print(f"è®­ç»ƒé›†: {len(train_dataset)} æ¡")
    print(f"éªŒè¯é›†: {len(eval_dataset)} æ¡")
    
    return train_dataset, eval_dataset


class FilteredDataset(Dataset):
    """è¿‡æ»¤æ— æ•ˆæ ·æœ¬çš„æ•°æ®é›†åŒ…è£…å™¨"""
    
    def __init__(self, original_dataset):
        self.original_dataset = original_dataset
        self.valid_indices = self._find_valid_indices()
    
    def _find_valid_indices(self):
        """æ‰¾åˆ°æ‰€æœ‰æœ‰æ•ˆæ ·æœ¬çš„ç´¢å¼•"""
        valid_indices = []
        print("æ‰«ææœ‰æ•ˆæ ·æœ¬...")
        
        for idx in range(len(self.original_dataset)):
            try:
                # å°è¯•è·å–æ ·æœ¬
                self.original_dataset[idx]
                valid_indices.append(idx)
            except Exception:
                # è·³è¿‡æ— æ•ˆæ ·æœ¬
                continue
        
        print(f"æ‰¾åˆ° {len(valid_indices)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        return valid_indices
    
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        # æ˜ å°„åˆ°åŸå§‹æ•°æ®é›†çš„ç´¢å¼•
        original_idx = self.valid_indices[idx]
        return self.original_dataset[original_idx]