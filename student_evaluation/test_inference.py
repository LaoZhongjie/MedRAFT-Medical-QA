"""
æ¨ç†æµ‹è¯•è„šæœ¬ (ç®€åŒ–ç‰ˆ)
éšæœºæŠ½å– teacher_dataset.json ä¸­çš„ 100 æ¡æ ·æœ¬è¿›è¡Œæ¨ç†æµ‹è¯•
"""

import json
import random
from config import ModelConfig, InferenceConfig
from inference import load_and_test_model


def load_random_samples(json_path: str, sample_size: int = 100):
    """ä» JSON æ–‡ä»¶ä¸­éšæœºæŠ½å–æ ·æœ¬"""
    print(f"æ­£åœ¨åŠ è½½æ•°æ®é›†: {json_path}")
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if not isinstance(data, list):
        raise ValueError("teacher_dataset.json å¿…é¡»æ˜¯åŒ…å«å¤šä¸ªæ ·æœ¬çš„åˆ—è¡¨ç»“æ„")

    if len(data) < sample_size:
        print(f"âš ï¸ æ•°æ®é›†æ ·æœ¬æ•° ({len(data)}) å°‘äº {sample_size}ï¼Œå°†å…¨éƒ¨ä½¿ç”¨ã€‚")
        sample_size = len(data)

    samples = random.sample(data, sample_size)
    print(f"âœ… å·²éšæœºæŠ½å– {sample_size} æ¡æ ·æœ¬è¿›è¡Œæµ‹è¯•\n")
    return samples


def main():
    """ä¸»å‡½æ•°"""
    # å›ºå®šæ–‡ä»¶è·¯å¾„
    dataset_path = "teacher_dataset.json"
    model_path = "output/final_model"  # ä½ è‡ªå·±çš„æ¨¡å‹è·¯å¾„
    base_model = "Qwen/Qwen2.5-7B-Instruct"

    print("\n" + "="*60)
    print("RAFT æ¨¡å‹æ‰¹é‡æ¨ç†æµ‹è¯•")
    print("="*60 + "\n")

    # åˆ›å»ºé…ç½®
    model_config = ModelConfig(
        model_name_or_path=base_model,
        torch_dtype='bfloat16'
    )

    inference_config = InferenceConfig(
        max_new_tokens=1024,
        temperature=0.7,
        top_p=0.9
    )

    # åŠ è½½å¹¶æŠ½æ ·
    test_samples = load_random_samples(dataset_path, sample_size=2)

    # æ‰§è¡Œæ¨ç†æµ‹è¯•
    for i, sample in enumerate(test_samples, 1):
        print(f"\nğŸ§  æµ‹è¯•æ ·æœ¬ {i}/{len(test_samples)} - é—®é¢˜: {sample['question']}")
        load_and_test_model(
            model_path=model_path,
            test_sample=sample,
            model_config=model_config,
            inference_config=inference_config
        )


if __name__ == "__main__":
    main()
