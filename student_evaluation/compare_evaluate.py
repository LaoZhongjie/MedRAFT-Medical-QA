import json
import csv
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from evaluate import load
import spacy
import re
import jieba
import warnings

warnings.filterwarnings("ignore", message="Failed to load image Python extension")

# --- åŠ è½½ SciSpacy ---
try:
    nlp = spacy.load("en_core_sci_sm")
except:
    nlp = spacy.load("en_core_web_sm")

# --- å·¥å…·å‡½æ•° ---
def extract_medical_entities(text: str):
    doc = nlp(text)
    return set([ent.text.lower() for ent in doc.ents])

def calculate_format_score(text: str):
    """æŒ‰ç…§ prompt çš„ 6 ä¸ªéƒ¨åˆ†è®¡ç®—æ ¼å¼åˆ†"""
    score = 0
    if "é—®é¢˜" in text: score += 1
    if "å‡è®¾" in text or "å·²çŸ¥ä¿¡æ¯" in text: score += 1
    if "CoTæ¨ç†" in text: score += 1
    if "åˆæ­¥è¯Šæ–­å»ºè®®" in text: score += 1
    if "è¯æ®å¼•ç”¨" in text: score += 1
    if "ä¸è¶³ä¿¡æ¯" in text or "åç»­å»ºè®®" in text: score += 1
    return score / 6

def f1(pred, ref):
    if not ref: return 0.0
    tp = len(pred & ref)
    precision = tp / len(pred) if pred else 0
    recall = tp / len(ref)
    return 2 * precision * recall / (precision + recall + 1e-8)

# --- jieba + è‹±æ–‡åˆ†è¯ + å»æ‰æ ‡ç‚¹ ---
def jieba_mixed_tokenize(text: str):
    tokens = []
    # åŒ¹é…ä¸­æ–‡è¿ç»­å­—ç¬¦å—ã€è‹±æ–‡å•è¯æˆ–æ•°å­—
    parts = re.findall(r'[\u4e00-\u9fff]+|[A-Za-z0-9]+', text)
    for part in parts:
        if re.match(r'[\u4e00-\u9fff]+', part):  # ä¸­æ–‡å—
            tokens.extend(jieba.lcut(part))
        else:  # è‹±æ–‡æˆ–æ•°å­—
            tokens.append(part)
    return tokens

# --- ä¸»é‡åŒ–å‡½æ•° ---
def quantitative_comparison(test_samples):
    rouge = load("rouge")
    bleu = load("bleu")
    bertscore = load("bertscore")

    metrics = {
        "rougeL": {"base": [], "tuned": []},
        "bleu": {"base": [], "tuned": []},
        "bertscore": {"base": [], "tuned": []},
        "entity_f1": {"base": [], "tuned": []},
        "format_score": {"base": [], "tuned": []},
        "response_length": {"base": [], "tuned": []}
    }

    detailed_rows = []

    for sample in tqdm(test_samples, desc="Evaluating samples"):
        teacher = sample.get("teacher_answer", "")
        base_resp = sample["base_response"]
        tuned_resp = sample["tuned_response"]

        # print('-'*100, type(base_resp))
        # ========== åˆ†è¯åçš„ BLEU & ROUGE ==========
        # base_tokens = jieba_mixed_tokenize(base_resp)
        # tuned_tokens = jieba_mixed_tokenize(tuned_resp)
        # teacher_tokens = jieba_mixed_tokenize(teacher)

        # # æ‹¼æˆç©ºæ ¼åˆ†éš”å­—ç¬¦ä¸²ç”¨äº ROUGE
        # base_str = " ".join(base_tokens)
        # tuned_str = " ".join(tuned_tokens)
        # teacher_str = " ".join(teacher_tokens)


        rouge_base = rouge.compute(predictions=[base_resp],
                                   references=[teacher])["rougeL"]
        rouge_tuned = rouge.compute(predictions=[tuned_resp],
                                    references=[teacher])["rougeL"]

        # BLEU å¯ä»¥ç›´æ¥ä¼  token åˆ—è¡¨
        bleu_base = bleu.compute(predictions=[base_resp],
                                 references=[teacher])["bleu"]
        bleu_tuned = bleu.compute(predictions=[tuned_resp],
                                  references=[teacher])["bleu"]

        # BERTScore ä¸­æ–‡
        bert_base = np.mean(bertscore.compute(predictions=[base_resp], references=[teacher], lang="zh")["f1"])
        bert_tuned = np.mean(bertscore.compute(predictions=[tuned_resp], references=[teacher], lang="zh")["f1"])

        # åŒ»å­¦å®ä½“
        base_entities = extract_medical_entities(base_resp)
        tuned_entities = extract_medical_entities(tuned_resp)
        ref_entities = extract_medical_entities(teacher)
        entity_base = f1(base_entities, ref_entities)
        entity_tuned = f1(tuned_entities, ref_entities)

        # æ ¼å¼åˆ†
        format_base = calculate_format_score(base_resp)
        format_tuned = calculate_format_score(tuned_resp)

        # å›ç­”é•¿åº¦ï¼ˆåˆ†è¯åé•¿åº¦ï¼‰
        len_base = len(base_resp)
        len_tuned = len(tuned_resp)

        # ä¿å­˜æ¯æ¡æ ·æœ¬
        detailed_rows.append({
            "id": sample.get("id", ""),
            "question": sample["question"],
            "rougeL_base": rouge_base,
            "rougeL_tuned": rouge_tuned,
            "bleu_base": bleu_base,
            "bleu_tuned": bleu_tuned,
            "bertscore_base": bert_base,
            "bertscore_tuned": bert_tuned,
            "entity_f1_base": entity_base,
            "entity_f1_tuned": entity_tuned,
            "format_score_base": format_base,
            "format_score_tuned": format_tuned,
            "response_length_base": len_base,
            "response_length_tuned": len_tuned
        })

        # ç´¯ç§¯å¹³å‡
        for key, val_base, val_tuned in [
            ("rougeL", rouge_base, rouge_tuned),
            ("bleu", bleu_base, bleu_tuned),
            ("bertscore", bert_base, bert_tuned),
            ("entity_f1", entity_base, entity_tuned),
            ("format_score", format_base, format_tuned),
            ("response_length", len_base, len_tuned)
        ]:
            metrics[key]["base"].append(val_base)
            metrics[key]["tuned"].append(val_tuned)

    # å¹³å‡æŒ‡æ ‡
    avg_results = {}
    for key, vals in metrics.items():
        base_mean = np.mean(vals["base"])
        tuned_mean = np.mean(vals["tuned"])
        improvement = (tuned_mean - base_mean) / (base_mean + 1e-8) * 100
        avg_results[key] = {
            "base": base_mean,
            "tuned": tuned_mean,
            "improvement": improvement
        }

    return avg_results, detailed_rows

# --- ä¿å­˜ CSV ---
def save_detailed_csv(detailed_rows, filename="detailed_metrics.csv"):
    df = pd.DataFrame(detailed_rows)
    df.to_csv(filename, index=False)
    print(f"âœ… æ¯æ¡æ ·æœ¬æŒ‡æ ‡å·²ä¿å­˜åˆ° {filename}")

# --- å¯è§†åŒ– ---
def plot_metrics(avg_results, detailed_rows, savepath="results"):
    """æ”¹è¿›çš„å¯è§†åŒ–ï¼šæŒ‰æŒ‡æ ‡ç±»å‹åˆ†ç»„ç»˜åˆ¶"""
    
    # 1. åˆ†ç±»æŒ‡æ ‡
    similarity_metrics = ["rougeL", "bleu", "bertscore", "entity_f1", "format_score"]
    length_metrics = ["response_length"]
    
    # 2. æå–æ•°æ®
    sim_base = [avg_results[m]["base"] for m in similarity_metrics]
    sim_tuned = [avg_results[m]["tuned"] for m in similarity_metrics]
    
    len_base = [avg_results[m]["base"] for m in length_metrics]
    len_tuned = [avg_results[m]["tuned"] for m in length_metrics]
    
    improvements = [avg_results[m]["improvement"] for m in similarity_metrics + length_metrics]
    all_metrics = similarity_metrics + length_metrics
    
    # ========== å›¾1: ç›¸ä¼¼åº¦æŒ‡æ ‡å¯¹æ¯”ï¼ˆæŸ±çŠ¶å›¾ï¼‰==========
    fig, ax = plt.subplots(figsize=(10, 5))
    x = np.arange(len(similarity_metrics))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, sim_base, width, label="Base", color='#3498db', alpha=0.8)
    bars2 = ax.bar(x + width/2, sim_tuned, width, label="Tuned", color='#e74c3c', alpha=0.8)
    
    ax.set_xlabel('Metrics', fontsize=11, fontweight='bold')
    ax.set_ylabel('Score', fontsize=11, fontweight='bold')
    ax.set_title('Similarity Metrics Comparison (0-1 Scale)', fontsize=13, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(similarity_metrics, rotation=15, ha='right')
    ax.legend(fontsize=10)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.set_ylim(0, 1.0)
    
    # åœ¨æŸ±å­ä¸Šæ ‡æ³¨æ•°å€¼
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    plt.savefig(f"{savepath}/similarity_bar.png", dpi=300)
    plt.close()
    
    # ========== å›¾2: é•¿åº¦æŒ‡æ ‡å¯¹æ¯”ï¼ˆå•ç‹¬å›¾ï¼‰==========
    fig, ax = plt.subplots(figsize=(6, 5))
    x = np.arange(len(length_metrics))
    width = 0.35
    
    bars1 = ax.bar(x - width/2, len_base, width, label="Base", color='#3498db', alpha=0.8)
    bars2 = ax.bar(x + width/2, len_tuned, width, label="Tuned", color='#e74c3c', alpha=0.8)
    
    ax.set_xlabel('Metric', fontsize=11, fontweight='bold')
    ax.set_ylabel('Length (characters)', fontsize=11, fontweight='bold')
    ax.set_title('Response Length Comparison', fontsize=13, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(length_metrics)
    ax.legend(fontsize=10)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    
    # æ ‡æ³¨æ•°å€¼
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(f"{savepath}/length_bar.png", dpi=300)
    plt.close()
    
    # ========== å›¾3: æ”¹è¿›ç™¾åˆ†æ¯”ï¼ˆæ¨ªå‘æŸ±çŠ¶å›¾ï¼‰==========
    fig, ax = plt.subplots(figsize=(10, 6))
    
    colors = ['#2ecc71' if imp > 0 else '#e74c3c' for imp in improvements]
    bars = ax.barh(all_metrics, improvements, color=colors, alpha=0.8)
    
    ax.set_xlabel('Improvement (%)', fontsize=11, fontweight='bold')
    ax.set_title('Performance Improvement: Tuned vs Base', fontsize=13, fontweight='bold')
    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
    ax.grid(axis='x', alpha=0.3, linestyle='--')
    
    # æ ‡æ³¨ç™¾åˆ†æ¯”
    for i, (bar, val) in enumerate(zip(bars, improvements)):
        ax.text(val + (2 if val > 0 else 8), i, f'{val:+.1f}%', 
               va='center', ha='left' if val > 0 else 'right', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(f"{savepath}/improvement.png", dpi=300)
    plt.close()
    
    # ========== å›¾4: å¤šæŒ‡æ ‡çƒ­åŠ›å›¾ï¼ˆæ ‡å‡†åŒ–ï¼‰==========
    # æ ‡å‡†åŒ–åˆ°0-1ï¼Œä½¿ä¸åŒé‡çº§çš„æŒ‡æ ‡å¯æ¯”
    heatmap_data = []
    for metric in all_metrics:
        base_val = avg_results[metric]["base"]
        tuned_val = avg_results[metric]["tuned"]
        
        # æ ‡å‡†åŒ–ï¼šå¦‚æœæ˜¯é•¿åº¦ï¼Œå…ˆç¼©æ”¾
        if metric in length_metrics:
            max_val = max(base_val, tuned_val)
            base_norm = base_val / max_val if max_val > 0 else 0
            tuned_norm = tuned_val / max_val if max_val > 0 else 0
        else:
            base_norm = base_val
            tuned_norm = tuned_val
        
        heatmap_data.append([base_norm, tuned_norm])
    
    df_heat = pd.DataFrame(heatmap_data, columns=["Base", "Tuned"], index=all_metrics)
    
    fig, ax = plt.subplots(figsize=(5, 7))
    sns.heatmap(df_heat, annot=True, fmt=".3f", cmap="RdYlGn", 
                vmin=0, vmax=1, cbar_kws={'label': 'Normalized Score'},
                linewidths=0.5, ax=ax)
    ax.set_title('Normalized Metrics Heatmap', fontsize=13, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f"{savepath}/heatmap_normalized.png", dpi=300)
    plt.close()
    
    # ========== å›¾5: æ ·æœ¬çº§æŒ‡æ ‡åˆ†å¸ƒï¼ˆç®±çº¿å›¾ï¼‰==========
    df = pd.DataFrame(detailed_rows)
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, metric in enumerate(similarity_metrics):
        ax = axes[idx]
        data_to_plot = [df[f"{metric}_base"], df[f"{metric}_tuned"]]
        bp = ax.boxplot(data_to_plot, labels=["Base", "Tuned"], patch_artist=True)
        
        # ç¾åŒ–ç®±çº¿å›¾
        for patch, color in zip(bp['boxes'], ['#3498db', '#e74c3c']):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        ax.set_ylabel('Score', fontsize=9)
        ax.set_title(f'{metric.upper()} Distribution', fontsize=10, fontweight='bold')
        ax.grid(axis='y', alpha=0.3, linestyle='--')
    
    # åˆ é™¤å¤šä½™å­å›¾
    if len(similarity_metrics) < len(axes):
        for idx in range(len(similarity_metrics), len(axes)):
            fig.delaxes(axes[idx])
    
    plt.tight_layout()
    plt.savefig(f"{savepath}/boxplot.png", dpi=300)
    plt.close()
    
    # ========== å›¾6: æ•£ç‚¹å›¾çŸ©é˜µï¼ˆæ ·æœ¬çº§å¯¹æ¯”ï¼‰==========
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()
    
    for idx, metric in enumerate(similarity_metrics):
        ax = axes[idx]
        base_col = f"{metric}_base"
        tuned_col = f"{metric}_tuned"
        
        ax.scatter(df[base_col], df[tuned_col], alpha=0.6, s=30, color='#9b59b6')
        
        # å¯¹è§’çº¿ y=x
        max_val = max(df[base_col].max(), df[tuned_col].max())
        min_val = min(df[base_col].min(), df[tuned_col].min())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=1.5, label='y=x')
        
        ax.set_xlabel('Base Score', fontsize=9)
        ax.set_ylabel('Tuned Score', fontsize=9)
        ax.set_title(f'{metric.upper()} Sample-wise', fontsize=10, fontweight='bold')
        ax.legend(fontsize=8)
        ax.grid(alpha=0.3, linestyle='--')
    
    if len(similarity_metrics) < len(axes):
        for idx in range(len(similarity_metrics), len(axes)):
            fig.delaxes(axes[idx])
    
    plt.tight_layout()
    plt.savefig(f"{savepath}/scatter_matrix.png", dpi=300)
    plt.close()
    
    # ========== å›¾7: é›·è¾¾å›¾ï¼ˆæ•´ä½“å¯¹æ¯”ï¼‰==========
    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='polar'), constrained_layout=True)
    
    angles = np.linspace(0, 2 * np.pi, len(similarity_metrics), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ
    
    sim_base_plot = sim_base + [sim_base[0]]
    sim_tuned_plot = sim_tuned + [sim_tuned[0]]
    
    ax.plot(angles, sim_base_plot, 'o-', linewidth=2, label='Base', color='#3498db')
    ax.fill(angles, sim_base_plot, alpha=0.25, color='#3498db')
    
    ax.plot(angles, sim_tuned_plot, 'o-', linewidth=2, label='Tuned', color='#e74c3c')
    ax.fill(angles, sim_tuned_plot, alpha=0.25, color='#e74c3c')
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(similarity_metrics, fontsize=10)
    ax.set_ylim(0, 1)
    ax.set_title('Similarity Metrics Radar Chart', fontsize=13, fontweight='bold', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.05, 1.05))
    ax.grid(True)
    
    plt.tight_layout()
    plt.savefig(f"{savepath}/radar.png", dpi=300)
    plt.close()
    
    print("âœ… æ”¹è¿›ç‰ˆå›¾è¡¨å·²ç”Ÿæˆï¼š")
    print(f"   1. {savepath}/similarity_bar.png - ç›¸ä¼¼åº¦æŒ‡æ ‡å¯¹æ¯”")
    print(f"   2. {savepath}/length_bar.png - é•¿åº¦æŒ‡æ ‡å¯¹æ¯”")
    print(f"   3. {savepath}/improvement.png - æ”¹è¿›ç™¾åˆ†æ¯”")
    print(f"   4. {savepath}/heatmap_normalized.png - æ ‡å‡†åŒ–çƒ­åŠ›å›¾")
    print(f"   5. {savepath}/boxplot.png - æ ·æœ¬åˆ†å¸ƒç®±çº¿å›¾")
    print(f"   6. {savepath}/scatter_matrix.png - æ ·æœ¬æ•£ç‚¹çŸ©é˜µ")
    print(f"   7. {savepath}/radar.png - é›·è¾¾å›¾")

# --- main ---
def main():
    with open("results/compare_results.json", "r", encoding="utf-8") as f:
        test_samples = json.load(f)

    avg_results, detailed_rows = quantitative_comparison(test_samples)
    save_detailed_csv(detailed_rows)
    plot_metrics(avg_results, detailed_rows)

    # æ‰“å°å¹³å‡æŠ¥å‘Š
    print("\nğŸ“Š å¹³å‡æŒ‡æ ‡æŠ¥å‘Š")
    for metric, vals in avg_results.items():
        print(f"{metric}: Base={vals['base']:.4f}, Tuned={vals['tuned']:.4f}, Improvement={vals['improvement']:+.2f}%")

if __name__ == "__main__":
    main()
