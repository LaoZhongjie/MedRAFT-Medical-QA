# RAFT çŸ¥è¯†è’¸é¦è®­ç»ƒç³»ç»Ÿ

ä¸€ä¸ªå®Œæ•´çš„çŸ¥è¯†è’¸é¦è®­ç»ƒå·¥ç¨‹,ç”¨äºè®­ç»ƒå­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„æ¨ç†é£æ ¼ã€‚æ”¯æŒ Qwenã€LLaMA ç­‰ä¸»æµå¤§è¯­è¨€æ¨¡å‹ã€‚

## ğŸ“‹ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ config.py              # é…ç½®ç®¡ç†æ¨¡å—
â”œâ”€â”€ dataset.py             # æ•°æ®é›†å¤„ç†æ¨¡å—
â”œâ”€â”€ model.py               # æ¨¡å‹åŠ è½½ä¸é…ç½®æ¨¡å—
â”œâ”€â”€ trainer.py             # è®­ç»ƒå™¨æ¨¡å—
â”œâ”€â”€ inference.py           # æ¨ç†æ¨¡å—
â”œâ”€â”€ utils.py               # å·¥å…·å‡½æ•°æ¨¡å—
â”œâ”€â”€ main.py                # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ test_inference.py      # æ¨ç†æµ‹è¯•è„šæœ¬
â”œâ”€â”€ requirements.txt       # ä¾èµ–åŒ…åˆ—è¡¨
â””â”€â”€ README.md              # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¦‚æœä½¿ç”¨Flash Attention 2(å¯é€‰,å¯æå‡è®­ç»ƒé€Ÿåº¦)
pip install flash-attn --no-build-isolation
```

### 2. æ•°æ®å‡†å¤‡

å‡†å¤‡è®­ç»ƒæ•°æ®é›† `raft_dataset.json`,æ ¼å¼å¦‚ä¸‹:

```json
[
  {
    "question": "å¦‚æœæˆ‘æ¼æœæˆ–å¤šæœäº†æ²»ç–—2å‹ç³–å°¿ç—…çš„è¯,è¯¥æ€ä¹ˆåŠ?",
    "documents": [
      {
        "content": "2å‹ç³–å°¿ç—…è¯ç‰©æ²»ç–—æŒ‡å—...",
        "type": "oracle"
      },
      {
        "content": "å…¶ä»–ç›¸å…³æ–‡æ¡£...",
        "type": "distractor"
      }
    ],
    "teacher_answer": "- é—®é¢˜: ...\n- å‡è®¾/å·²çŸ¥ä¿¡æ¯: ...\n- CoTæ¨ç†:\n  1) ...\n  2) ...\n  3) ...\n- åˆæ­¥è¯Šæ–­å»ºè®®(å«ä¸ç¡®å®šåº¦): ...\n- è¯æ®å¼•ç”¨: ...\n- ä¸è¶³ä¿¡æ¯ä¸åç»­å»ºè®®: ...\n- ç´§æ€¥å°±åŒ»æŒ‡ç¤º(çº¢æ——ç—‡çŠ¶): ..."
  }
]
```

**ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†:**

```python
from utils import create_sample_dataset
create_sample_dataset("sample_dataset.json")
```

### 3. å¼€å§‹è®­ç»ƒ

**åŸºç¡€è®­ç»ƒå‘½ä»¤:**

```bash
python main.py \
  --train_file raft_dataset.json \
  --model_name Qwen/Qwen2.5-7B-Instruct \
  --output_dir ./output \
  --num_train_epochs 3 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --learning_rate 2e-4
```

**å®Œæ•´å‚æ•°ç¤ºä¾‹:**

```bash
python main.py \
  --train_file raft_dataset.json \
  --model_name Qwen/Qwen2.5-7B-Instruct \
  --output_dir ./output \
  --validation_split 0.1 \
  --num_train_epochs 3 \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8 \
  --learning_rate 2e-4 \
  --max_seq_length 4096 \
  --lora_r 64 \
  --lora_alpha 128 \
  --lora_dropout 0.05 \
  --seed 42
```

**ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ:**

```bash
python main.py \
  --train_file raft_dataset.json \
  --model_name Qwen/Qwen2.5-7B-Instruct \
  --output_dir ./output \
  --resume_from_checkpoint ./output/checkpoint-100
```

### 4. æ¨ç†æµ‹è¯•

**ä½¿ç”¨é»˜è®¤æµ‹è¯•æ ·æœ¬:**

```bash
python test_inference.py \
  --model_path ./output/final_model \
  --base_model Qwen/Qwen2.5-7B-Instruct
```

**ä½¿ç”¨è‡ªå®šä¹‰æµ‹è¯•æ–‡ä»¶:**

```bash
python test_inference.py \
  --model_path ./output/final_model \
  --test_file test_samples.json \
  --base_model Qwen/Qwen2.5-7B-Instruct
```

**ä½¿ç”¨å‘½ä»¤è¡Œé—®é¢˜æµ‹è¯•:**

```bash
python test_inference.py \
  --model_path ./output/final_model \
  --question "ä½ çš„é—®é¢˜" \
  --base_model Qwen/Qwen2.5-7B-Instruct
```

## âš™ï¸ é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½® (ModelConfig)

- `model_name_or_path`: åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„
- `torch_dtype`: æ•°æ®ç±»å‹ (float16/bfloat16/float32)
- `use_flash_attention_2`: æ˜¯å¦ä½¿ç”¨Flash Attention 2

### LoRA é…ç½® (LoRAConfig)

- `lora_r`: LoRA rank (å»ºè®® 16-128)
- `lora_alpha`: LoRA alpha (é€šå¸¸æ˜¯ r çš„ 2 å€)
- `lora_dropout`: Dropout ç‡
- `lora_target_modules`: ç›®æ ‡æ¨¡å—åˆ—è¡¨

### è®­ç»ƒé…ç½® (TrainingConfig)

- `num_train_epochs`: è®­ç»ƒè½®æ•°
- `per_device_train_batch_size`: æ¯è®¾å¤‡ batch size
- `gradient_accumulation_steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
- `learning_rate`: å­¦ä¹ ç‡
- `max_seq_length`: æœ€å¤§åºåˆ—é•¿åº¦
- `gradient_checkpointing`: æ˜¯å¦å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹(èŠ‚çœæ˜¾å­˜)

## ğŸ’¡ æ ¸å¿ƒç‰¹æ€§

### 1. LoRA å¾®è°ƒ
- ä½¿ç”¨ PEFT åº“å®ç°é«˜æ•ˆçš„ LoRA å¾®è°ƒ
- å¤§å¹…å‡å°‘è®­ç»ƒå‚æ•°å’Œæ˜¾å­˜å ç”¨
- æ”¯æŒè‡ªå®šä¹‰ LoRA é…ç½®

### 2. Gradient Checkpointing
- è‡ªåŠ¨å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
- é€‚åˆåœ¨æœ‰é™ GPU èµ„æºä¸‹è®­ç»ƒå¤§æ¨¡å‹

### 3. æ··åˆç²¾åº¦è®­ç»ƒ
- æ”¯æŒ BF16/FP16 æ··åˆç²¾åº¦è®­ç»ƒ
- æå‡è®­ç»ƒé€Ÿåº¦,å‡å°‘æ˜¾å­˜å ç”¨

### 4. æ–­ç‚¹ç»­è®­
- è‡ªåŠ¨ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
- æ”¯æŒä»ä»»æ„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

### 5. ç»“æ„åŒ–è¾“å‡º
- è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆç»“æ„åŒ–åŒ»ç–—å»ºè®®
- åŒ…å« CoT æ¨ç†ã€è¯æ®å¼•ç”¨ç­‰æ¨¡å—

### 6. è‡ªåŠ¨è¯„ä¼°
- è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
- ä¿å­˜æœ€ä½³æ¨¡å‹

## ğŸ“Š æ˜¾å­˜éœ€æ±‚

ä»¥ Qwen2.5-7B ä¸ºä¾‹:

| é…ç½® | Batch Size | æ¢¯åº¦ç´¯ç§¯ | æ˜¾å­˜éœ€æ±‚ |
|------|-----------|---------|---------|
| BF16 + LoRA64 | 1 | 8 | ~18GB |
| BF16 + LoRA64 | 2 | 8 | ~24GB |
| BF16 + LoRA64 | 4 | 4 | ~32GB |

**èŠ‚çœæ˜¾å­˜çš„æŠ€å·§:**
1. å¯ç”¨ `gradient_checkpointing`
2. å‡å° `per_device_train_batch_size`
3. å¢åŠ  `gradient_accumulation_steps`
4. å‡å° `max_seq_length`
5. ä½¿ç”¨è¾ƒå°çš„ `lora_r`

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æ•°æ®å¤„ç†

ä¿®æ”¹ `dataset.py` ä¸­çš„ `_build_prompt()` æ–¹æ³•æ¥è‡ªå®šä¹‰ prompt æ ¼å¼:

```python
def _build_prompt(self, item: Dict[str, Any]) -> str:
    # è‡ªå®šä¹‰ä½ çš„ prompt æ„å»ºé€»è¾‘
    question = item['question']
    documents = item['documents']
    # ...
    return prompt
```

### è‡ªå®šä¹‰è®­ç»ƒå›è°ƒ

åœ¨ `trainer.py` ä¸­æ‰©å±• `CustomCallback` ç±»:

```python
class CustomCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):
        # æ·»åŠ è‡ªå®šä¹‰é€»è¾‘
        pass
```

### ä½¿ç”¨ä¸åŒçš„åŸºç¡€æ¨¡å‹

æ”¯æŒä»»ä½• Hugging Face ä¸Šçš„ Causal LM æ¨¡å‹:

```bash
# LLaMA ç³»åˆ—
python main.py --model_name meta-llama/Llama-2-7b-hf ...

# Baichuan ç³»åˆ—
python main.py --model_name baichuan-inc/Baichuan2-7B-Base ...

# ChatGLM ç³»åˆ—
python main.py --model_name THUDM/chatglm3-6b ...
```

## ğŸ“ è¾“å‡ºè¯´æ˜

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šç”Ÿæˆä»¥ä¸‹è¾“å‡º:

```
output/
â”œâ”€â”€ checkpoint-100/          # è®­ç»ƒæ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint-200/
â”œâ”€â”€ final_model/             # æœ€ç»ˆæ¨¡å‹
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â””â”€â”€ ...
â”œâ”€â”€ logs/                    # TensorBoard æ—¥å¿—
â””â”€â”€ training_config.json     # è®­ç»ƒé…ç½®å¤‡ä»½
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. CUDA Out of Memory

**è§£å†³æ–¹æ¡ˆ:**
- å‡å° `per_device_train_batch_size`
- å¢åŠ  `gradient_accumulation_steps`
- å¯ç”¨ `gradient_checkpointing`
- å‡å° `max_seq_length`

### 2. è®­ç»ƒ Loss ä¸ä¸‹é™

**æ£€æŸ¥é¡¹:**
- å­¦ä¹ ç‡æ˜¯å¦åˆé€‚
- æ•°æ®é›†æ˜¯å¦æ­£ç¡®
- éªŒè¯ labels æ˜¯å¦æ­£ç¡®è®¾ç½®

### 3. ç”Ÿæˆç»“æœæ ¼å¼ä¸ç¬¦åˆé¢„æœŸ

**è§£å†³æ–¹æ¡ˆ:**
- å¢åŠ è®­ç»ƒæ•°æ®é‡
- å»¶é•¿è®­ç»ƒæ—¶é—´
- è°ƒæ•´ prompt æ¨¡æ¿
- åœ¨æ¨ç†æ—¶ä½¿ç”¨æ›´ä½çš„ temperature

## ğŸ“š å‚è€ƒèµ„æ–™

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [PEFT (Parameter-Efficient Fine-Tuning)](https://huggingface.co/docs/peft)
- [Qwen æ¨¡å‹](https://github.com/QwenLM/Qwen)

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request!

## âœ‰ï¸ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜,è¯·æäº¤ Issue æˆ–è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚